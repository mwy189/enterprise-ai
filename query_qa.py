import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from openai import OpenAI

# Embedding æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# DeepSeek å®¢æˆ·ç«¯ï¼ˆä¿æŒå…¨å±€ï¼Œä¸ç”¨æ¯æ¬¡éƒ½åˆ›å»ºï¼‰
client = OpenAI(
    api_key="sk-ab0bce7a83084d0896814ac560eafa73",
    base_url="https://api.deepseek.com"
)

# query å‡½æ•°
def query(role, question):
    # åŠ è½½å‘é‡åº“
    db = Chroma(
        persist_directory=f"vector_db/{role}",
        embedding_function=embeddings
    )

    # ç›¸ä¼¼åº¦æ£€ç´¢
    docs = db.similarity_search(question, k=3)
    print(f"ğŸ”¹ æ£€ç´¢åˆ° {len(docs)} æ¡æ–‡æ¡£")
    for i, d in enumerate(docs):
        print(f"æ–‡æ¡£{i}å†…å®¹: {d.page_content}")
    context = "\n".join([d.page_content for d in docs])

    # Prompt
    messages = [
        {"role": "system", "content": """
ä½ æ˜¯å…¬å¸å†…éƒ¨åˆ¶åº¦é—®ç­”åŠ©æ‰‹ã€‚
ä½ ã€åªèƒ½ã€‘æ ¹æ®æä¾›çš„èµ„æ–™å›ç­”é—®é¢˜ã€‚
å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œè¯·ç›´æ¥å›ç­”ï¼šèµ„æ–™ä¸­æœªæåŠï¼Œæ— æ³•ç¡®è®¤ã€‚
ä¸¥ç¦è¡¥å……ã€æ¨æµ‹ã€æ‰©å±•ä»»ä½•æœªåœ¨èµ„æ–™ä¸­å‡ºç°çš„å†…å®¹ã€‚
å›ç­”è¦ç®€æ´ã€å‡†ç¡®ï¼Œä¸ä½¿ç”¨é€šç”¨äººåŠ›èµ„æºå¸¸è¯†ã€‚
"""},

        {"role": "user", "content":f"""
ã€å…¬å¸åˆ¶åº¦èµ„æ–™ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”è¦æ±‚ã€‘
- ä»…ä½¿ç”¨èµ„æ–™ä¸­çš„ä¿¡æ¯
- ä¸å…è®¸å¼•å…¥å¤–éƒ¨å¸¸è¯†
- æœªæåŠå†…å®¹è¯·æ˜ç¡®è¯´æ˜æœªæåŠ
"""}
    ]

    # è°ƒç”¨ DeepSeek
    resp = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages
    )
    return resp.choices[0].message.content


if __name__ == "__main__":
    role = "HR"
    print("ä¼ä¸šåˆ¶åº¦é—®ç­”åŠ©æ‰‹å·²å¯åŠ¨ï¼Œè¾“å…¥ exit é€€å‡º")
    while True:
        question = input("\nè¯·è¾“å…¥é—®é¢˜ï¼š")
        if question.lower() in ["exit", "quit"]:
            print("é€€å‡ºé—®ç­”åŠ©æ‰‹")
            break
        answer = query(role, question)
        print("\nğŸ”¹ å›ç­”ï¼š")
        print(answer)
